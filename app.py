# app.py  â€“  å†å²ç›¸ä¼¼æ¯”èµ› Top5 æŸ¥çœ‹ä¸å¯¼å‡º
# + å†å² TOP5 çš„ 6 ä¸ªå·®å€¼æ¨¡å¼
# + æ¨¡å¼è®¡æ•°ä½“ç³»ï¼ˆequal_pro/diff1_pro/...ï¼‰
# + PRO_gap & PROèåˆæ¨¡å‹_gap ç­‰å·®é€’å¢ä¸‰å…ƒç»„æ£€æµ‹ï¼ˆæˆªæ–­ä¸¤ä½ï¼Œä¸å››èˆäº”å…¥ï¼‰
# + 5å®¶å…¬å¸ä¸»/å¹³/å®¢èµ”ç‡ï¼šè¿‘ä¼¼ç­‰å·®é€’å¢ä¸‰å…ƒç»„ï¼ˆæˆªæ–­ä¸¤ä½ï¼Œä¸å››èˆäº”å…¥ï¼›å…è®¸å·®å€¼å·®<=0.01ï¼›åˆ—å‡ºå…·ä½“ä¸‰å…ƒç»„ï¼‰
# + Top5 æ¯è¡Œè®¡ç®— gap_sum_100 = floor(PRO_gap*100) + floor(PROèåˆæ¨¡å‹_gap*100)
# + 199åº“ Top5 åºåˆ—åŒ¹é…ï¼ˆ>=4/5 ä¸”é¡ºåºä¸€è‡´ï¼‰ï¼Œå¹¶æ˜¾ç¤ºâ€œå½“å‰æ¯”èµ›ç»“æœâ€
# + ä¿ç•™ï¼šå¯¼å‡ºå†å²ç›¸ä¼¼TOP5ï¼ˆCSVï¼Œåˆ†ç»„ç©ºè¡Œï¼‰
# + æ–°å¢ï¼šTop3ï¼ˆTop5ç¬¬3è¡Œï¼‰è§„åˆ™æç¤ºï¼ˆ9æ¡ï¼‰
#   - Parity=0/1ï¼šç”±â€œæ•´ä½“ total_countâ€çš„å¥‡å¶å¾—åˆ°ï¼ˆå¶0å¥‡1ï¼‰
#   - â€œParity=3â€ï¼šå®é™…æŒ‡æ•´ä½“ total_count==3
#   - é¡µé¢å±•ç¤º Parity ç”¨ total_countï¼ˆä¸å±•ç¤º0/1ï¼‰

import math
import re
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any

import numpy as np
import pandas as pd
import streamlit as st

from models.pro_model import predict_model_pro
from models.model_pro_ensemble import predict_model_pro_ensemble
from models.predict_model_meta import predict_model_meta
from similarity_matcher import SimilarityMatcher

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI è®¾ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="å†å²ç›¸ä¼¼æ¯”èµ› Top5 å¯¼å‡º", layout="wide")

st.markdown("""
<style>
.stApp { background:linear-gradient(135deg,#f0f8ff,#e6e6fa); }
.company-name{font-size:1.05em;font-weight:600;text-shadow:1px 1px 2px rgba(0,0,0,0.2);}
.stTextInput>div>div>input{max-width:260px;}
.stButton>button{margin-top:4px;margin-bottom:8px;}

/* ç´§å‡‘å†å²è¡¨æ ¼æ ·å¼ */
.hist-table{
  table-layout: fixed;
  width: 100%;
  border-collapse: collapse;
}
.hist-table th, .hist-table td{
  max-width: 120px;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
  padding: 3px 4px;
  border-bottom: 1px solid #ddd;
  font-size: 0.85rem;
}
.hist-table th{
  font-weight: 600;
  background-color:#f5f5ff;
}

/* å·®å€¼æ¨¡å¼çš„å°è¡¨æ ¼ */
.pattern-table{
  border-collapse: collapse;
  margin-top: 4px;
}
.pattern-table th, .pattern-table td{
  border: 1px solid #ddd;
  padding: 3px 6px;
  font-size: 0.85rem;
}
.pattern-table th{
  background-color:#eef2ff;
  font-weight: 600;
}
</style>
""", unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŸºæœ¬å¸¸é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR  = Path(__file__).parent
DATA_FILE = BASE_DIR / "data" / "new_matches.xlsx"
HIST_PATH = BASE_DIR / "data" / "prediction_results (43).xlsx"

# 199åº“ï¼ˆæœªæ¥æ‰©å±•åˆ°300ä¹Ÿä¸€æ ·ï¼‰ç›¸å¯¹è·¯å¾„
REF_TOP5_PATH = BASE_DIR / "data" / "all_matches_top5_history.csv"

companies = ["Bet365", "ç«‹åš", "Interwetten", "Pinnacle", "William Hill"]
TEAM_COLS = ["ä¸»é˜Ÿ", "å®¢é˜Ÿ"]
outcomes = ["ä¸»èƒœ", "å¹³å±€", "å®¢èƒœ"]
ODDS_COLS = [f"{c}_{o}" for c in companies for o in outcomes]

# ---------- æ¸²æŸ“ç´§å‡‘è¡¨æ ¼ ----------
def render_compact_table(df: pd.DataFrame):
    """ç”¨å›ºå®šåˆ—å®½çš„ HTML è¡¨æ ¼æ˜¾ç¤º DataFrameï¼Œå¹¶ç»Ÿä¸€æ•°å€¼ä¸º 4 ä½å°æ•°ã€‚"""
    if df is None or df.empty:
        st.info("æš‚æ— æ•°æ®ã€‚")
        return
    html = df.to_html(
        index=False,
        classes="hist-table",
        border=0,
        escape=False,
        float_format=lambda x: f"{x:.4f}",
    )
    st.markdown(html, unsafe_allow_html=True)

# ---------- è¯†åˆ«â€œæ¯”èµ›ç»“æœâ€åˆ—å ----------
def get_result_value(row: pd.Series) -> str:
    """ä»ä¸€è¡Œä¸­å–æ¯”èµ›ç»“æœï¼šä¼˜å…ˆæ¯”èµ›ç»“æœï¼Œå…¶æ¬¡æ¯”èµ›ç»“æœ_yï¼Œå†å…¶æ¬¡æ¯”èµ›ç»“æœ_xã€‚"""
    for col in ["æ¯”èµ›ç»“æœ", "æ¯”èµ›ç»“æœ_y", "æ¯”èµ›ç»“æœ_x"]:
        if col in row and pd.notna(row[col]) and str(row[col]) != "":
            return str(row[col])
    return ""

# ---------- æˆªæ–­å·¥å…·ï¼šä¸¤ä½å°æ•°ï¼Œä¸å››èˆäº”å…¥ ----------
def _truncate_two_decimals(x: float) -> float:
    """æˆªæ–­åˆ°ä¸¤ä½å°æ•°ï¼ˆä¸å››èˆäº”å…¥ï¼‰ã€‚"""
    try:
        x = float(x)
    except (TypeError, ValueError):
        return 0.0
    if x >= 0:
        return math.floor(x * 100 + 1e-8) / 100.0
    else:
        return math.ceil(x * 100 - 1e-8) / 100.0

def _floor_times_100(x: float) -> int:
    """å¯¹æ­£æ•°ç­‰ä»·äº floor(x*100)ã€‚"""
    try:
        x = float(x)
    except (TypeError, ValueError):
        return 0
    if pd.isna(x):
        return 0
    return int(math.floor(x * 100 + 1e-8))

def compute_gap_sum_100(pro_gap: float, ens_gap: float) -> int:
    """floor(PRO_gap*100) + floor(ENS_gap*100)"""
    return _floor_times_100(pro_gap) + _floor_times_100(ens_gap)

# ---------- å·®å€¼æ¨¡å¼å·¥å…· ----------
def format_gap_pattern(values) -> str:
    """3 ä¸ª gap â†’ '(outer)d1-d2' æ¨¡å¼å­—ç¬¦ä¸²ã€‚"""
    if len(values) != 3:
        return ""
    vs = sorted(_truncate_two_decimals(v) for v in values)
    d1 = int(math.floor(abs(vs[1] - vs[0]) * 100 + 1e-8))
    d2 = int(math.floor(abs(vs[2] - vs[1]) * 100 + 1e-8))
    if (d1 == 0 and d2 != 0) or (d2 == 0 and d1 != 0):
        first, second = 0, d2 if d1 == 0 else d1
    else:
        first, second = d1, d2
    outer = abs(first - second)
    return f"({outer}){first}-{second}"

def compute_gap_patterns(sims: pd.DataFrame, col: str) -> dict:
    """ä¸ºæŒ‡å®š gap åˆ—è®¡ç®— 0-1-2 / 1-2-3 / 2-3-4 ä¸‰ä¸ªçª—å£çš„å·®å€¼æ¨¡å¼ã€‚"""
    if col not in sims.columns:
        return {}
    vals = sims[col].tolist()
    patterns = {}
    for label, start in [("0-1-2", 0), ("1-2-3", 1), ("2-3-4", 2)]:
        if len(vals) >= start + 3:
            patterns[label] = format_gap_pattern(vals[start:start+3])
        else:
            patterns[label] = ""
    return patterns

# ---------- PRO_gap & ENS_gap ç­‰å·®ä¸‰å…ƒç»„å·¥å…· ----------
def compute_gap_ap(sims: pd.DataFrame, col: str):
    """Top5 gap æˆªæ–­ä¸¤ä½åï¼Œæ‰¾ä¸¥æ ¼é€’å¢ç­‰å·®ä¸‰å…ƒç»„ï¼›best å–å…¬å·®æœ€å°çš„ä¸€ç»„ã€‚"""
    if col not in sims.columns or sims.empty:
        return [], 0, None, []

    gaps = sims[col].tolist()
    trunc = [_truncate_two_decimals(x) for x in gaps]

    ints = [int(round(v * 100)) for v in trunc]
    uniq = sorted(set(ints))

    has_ap = 0
    best_triplet = None
    best_step = None
    all_triplets: List[Tuple[float, float, float]] = []

    n = len(uniq)
    for i in range(n):
        for j in range(i + 1, n):
            for k in range(j + 1, n):
                a, b, c = uniq[i], uniq[j], uniq[k]
                if b > a and c > b and (b - a) == (c - b):
                    has_ap = 1
                    step = b - a
                    triplet = (a / 100.0, b / 100.0, c / 100.0)
                    all_triplets.append(triplet)
                    if best_step is None or step < best_step:
                        best_step = step
                        best_triplet = triplet

    return trunc, has_ap, best_triplet, all_triplets

# ---------- æ¨¡å¼è§£æ & è®¡æ•°ä½“ç³»ï¼ˆè¿”å› total_count + parity_bitï¼‰ ----------
def parse_pair_from_pattern(pat: str) -> Optional[Tuple[int, int]]:
    if not isinstance(pat, str) or not pat.strip():
        return None
    nums = re.findall(r"\d+", pat)
    if len(nums) < 2:
        return None
    return int(nums[-2]), int(nums[-1])

def delta_from_pattern(pat: str) -> Optional[int]:
    pair = parse_pair_from_pattern(pat)
    if pair is None:
        return None
    a, b = pair
    return abs(b - a)

def compute_pattern_counts_for_match(
    pro0: str, pro1: str, pro2: str,
    ens0: str, ens1: str, ens2: str,
):
    """
    è¿”å›ï¼š
      total_count = è®¡æ•°æ€»æ¬¡æ•°ï¼ˆæ•´æ•°ï¼‰
      parity_bit = total_countå¥‡å¶ï¼ˆå¶0å¥‡1ï¼‰
    """
    pro_pats = [pro0, pro1, pro2]
    ens_pats = [ens0, ens1, ens2]

    pro_pairs = [parse_pair_from_pattern(p) for p in pro_pats]
    pro_deltas = [delta_from_pattern(p) for p in pro_pats]

    ens_pairs = [parse_pair_from_pattern(p) for p in ens_pats]
    ens_deltas = [delta_from_pattern(p) for p in ens_pats]

    equal_pro = 0
    diff1_pro = 0
    for i in range(3):
        for j in range(i + 1, 3):
            di, dj = pro_deltas[i], pro_deltas[j]
            pi, pj = pro_pairs[i], pro_pairs[j]
            if di is None or dj is None or pi is None or pj is None:
                continue
            if di == dj:
                equal_pro += 1
            if abs(di - dj) == 1 and (set(pi) & set(pj)):
                diff1_pro += 1

    equal_ens = 0
    diff1_ens = 0
    for i in range(3):
        for j in range(i + 1, 3):
            di, dj = ens_deltas[i], ens_deltas[j]
            pi, pj = ens_pairs[i], ens_pairs[j]
            if di is None or dj is None or pi is None or pj is None:
                continue
            if di == dj:
                equal_ens += 1
            if abs(di - dj) == 1 and (set(pi) & set(pj)):
                diff1_ens += 1

    equal_cross = 0
    diff1_cross = 0
    for i in range(3):
        for j in range(3):
            di, dj = pro_deltas[i], ens_deltas[j]
            pi, pj = pro_pairs[i], ens_pairs[j]
            if di is None or dj is None or pi is None or pj is None:
                continue
            if di == dj:
                equal_cross += 1
            if abs(di - dj) == 1 and not (set(pi) & set(pj)):
                diff1_cross += 1

    total = equal_pro + diff1_pro + equal_ens + diff1_ens + equal_cross + diff1_cross
    parity_bit = 1 if (total % 2) == 1 else 0

    return {
        "equal_pro": equal_pro,
        "diff1_pro": diff1_pro,
        "equal_ens": equal_ens,
        "diff1_ens": diff1_ens,
        "equal_cross": equal_cross,
        "diff1_cross": diff1_cross,
        "total_count": total,
        "parity_bit": parity_bit,
    }

# ========== èµ”ç‡ï¼ˆä¸‰å…ƒç»„è¿‘ä¼¼ç­‰å·®ï¼‰å·¥å…· ==========
def find_ap_triplets_for_odds(company_odds: List[Tuple[str, float]], tolerance_ticks: int = 1):
    trunc_list = [(c, _truncate_two_decimals(v)) for c, v in company_odds]

    triplets_desc: List[str] = []
    for a in range(5):
        for b in range(a + 1, 5):
            for c in range(b + 1, 5):
                t = [trunc_list[a], trunc_list[b], trunc_list[c]]
                t_sorted = sorted(t, key=lambda x: x[1])

                v1, v2, v3 = t_sorted[0][1], t_sorted[1][1], t_sorted[2][1]
                if not (v1 < v2 < v3):
                    continue

                i1, i2, i3 = int(round(v1 * 100)), int(round(v2 * 100)), int(round(v3 * 100))
                d1_ticks = i2 - i1
                d2_ticks = i3 - i2

                if abs(d1_ticks - d2_ticks) <= tolerance_ticks:
                    d1 = d1_ticks / 100.0
                    d2 = d2_ticks / 100.0
                    delta = abs(d1_ticks - d2_ticks) / 100.0
                    triplets_desc.append(
                        f"{t_sorted[0][0]}:{v1:.2f} < {t_sorted[1][0]}:{v2:.2f} < {t_sorted[2][0]}:{v3:.2f}"
                        f" | d1={d1:.2f} d2={d2:.2f} |Î”|={delta:.2f}"
                    )

    trunc_vals = [v for _, v in trunc_list]
    return trunc_vals, triplets_desc

def analyze_odds_ap_for_match(input_row: pd.Series, tolerance_ticks: int = 1):
    result = {}
    for outcome in ["ä¸»èƒœ", "å¹³å±€", "å®¢èƒœ"]:
        company_odds = []
        for comp in companies:
            col = f"{comp}_{outcome}"
            company_odds.append((comp, float(input_row[col]) if pd.notna(input_row[col]) else 0.0))

        trunc_vals, trips = find_ap_triplets_for_odds(company_odds, tolerance_ticks=tolerance_ticks)
        key_prefix = {"ä¸»èƒœ": "H", "å¹³å±€": "D", "å®¢èƒœ": "A"}[outcome]

        result[f"{key_prefix}_odds_trunc"] = ",".join(f"{v:.2f}" for v in trunc_vals)
        result[f"{key_prefix}_odds_ap_has"] = 1 if trips else 0
        result[f"{key_prefix}_odds_ap_count"] = len(trips)
        result[f"{key_prefix}_odds_ap_triplets"] = "ï¼› ".join(trips)

    return result

# ========== å‚è€ƒåº“ Top5åºåˆ—åŒ¹é…ï¼ˆå¸¦â€œå½“å‰æ¯”èµ›ç»“æœâ€ï¼‰ ==========
def _detect_col(df: pd.DataFrame, candidates: List[str]) -> str:
    for c in candidates:
        if c in df.columns:
            return c
    return ""

@st.cache_data
def load_ref_top5_map_with_result(ref_path: Path) -> Dict[int, Dict[str, Any]]:
    if not ref_path.exists():
        return {}

    df = pd.read_csv(ref_path)
    cur_col = _detect_col(df, ["å½“å‰æ¯”èµ›åºå·"])
    hist_col = _detect_col(df, ["å†å²æ¯”èµ›åºå·"])
    res_col = _detect_col(df, ["å½“å‰æ¯”èµ›ç»“æœ"])

    if cur_col == "" or hist_col == "":
        return {}

    df = df.dropna(subset=[cur_col, hist_col]).copy()
    df[cur_col] = pd.to_numeric(df[cur_col], errors="coerce")
    df[hist_col] = pd.to_numeric(df[hist_col], errors="coerce")
    df = df.dropna(subset=[cur_col, hist_col]).copy()
    df[cur_col] = df[cur_col].astype(int)
    df[hist_col] = df[hist_col].astype(int)

    ref_map: Dict[int, Dict[str, Any]] = {}
    for _, r in df.iterrows():
        k = int(r[cur_col])
        v = int(r[hist_col])

        if k not in ref_map:
            ref_map[k] = {"seq": [], "result": ""}

        if len(ref_map[k]["seq"]) < 5:
            ref_map[k]["seq"].append(v)

        if res_col != "" and ref_map[k]["result"] == "":
            val = r.get(res_col, "")
            if pd.notna(val) and str(val).strip() != "":
                ref_map[k]["result"] = str(val).strip()

    for k in list(ref_map.keys()):
        if len(ref_map[k]["seq"]) != 5:
            ref_map.pop(k, None)

    return ref_map

def match_top5_sequence(new_seq: List[int], ref_map: Dict[int, Dict[str, Any]]) -> List[Dict[str, Any]]:
    results: List[Dict[str, Any]] = []
    if len(new_seq) != 5 or not ref_map:
        return results

    new_tuple = tuple(new_seq)
    new_sub4 = [tuple([new_seq[j] for j in range(5) if j != drop_i]) for drop_i in range(5)]

    for ref_match_no, obj in ref_map.items():
        ref_seq = obj.get("seq", [])
        ref_res = obj.get("result", "")
        if len(ref_seq) != 5:
            continue

        ref_tuple = tuple(ref_seq)
        if ref_tuple == new_tuple:
            results.append({
                "ref_match_no": ref_match_no,
                "level": 5,
                "ref_seq": ref_seq,
                "ref_result": ref_res,
                "matched_subseq": None,
            })
            continue

        ref_sub4_set = set(tuple([ref_seq[k] for k in range(5) if k != drop_j]) for drop_j in range(5))
        hit_sub = None
        for sub in new_sub4:
            if sub in ref_sub4_set:
                hit_sub = list(sub)
                break

        if hit_sub is not None:
            results.append({
                "ref_match_no": ref_match_no,
                "level": 4,
                "ref_seq": ref_seq,
                "ref_result": ref_res,
                "matched_subseq": hit_sub,
            })

    results.sort(key=lambda x: (-x["level"], x["ref_match_no"]))
    return results

# ========== Top3 è§„åˆ™æç¤ºï¼ˆä½¿ç”¨â€œæ•´ä½“ total_count/parity_bitâ€ï¼Œä¸æ˜¯Top3è¡Œçš„Parityï¼‰ ==========
def _in_range(x: Optional[float], lo: float, hi: float) -> bool:
    if x is None or pd.isna(x):
        return False
    return (x >= lo) and (x <= hi)

def check_top3_rules(
    sims_basic_full: pd.DataFrame,
    overall_total_count: Optional[int],
    overall_parity_bit: Optional[int],
) -> List[str]:
    """
    åŸºäºTop3ï¼ˆTop5ç¬¬3è¡Œï¼‰è¯»å–æ¨¡å‹é¢„æµ‹ä¸ens_gapï¼Œ
    ä½† Parity=0/1/3 å…¨éƒ¨æ¥è‡ªâ€œæ•´ä½“ total_count/parity_bitâ€ï¼š
      - Parity=0/1ï¼šoverall_parity_bitï¼ˆå¶0å¥‡1ï¼‰
      - Parity=3ï¼šoverall_total_count==3
    """
    msgs: List[str] = []
    if sims_basic_full is None or sims_basic_full.empty or len(sims_basic_full) < 3:
        return msgs

    top3 = sims_basic_full.iloc[2]

    pro_pred = str(top3.get("PRO_æœ€ç»ˆé¢„æµ‹ç»“æœ", "")).strip()
    ens_pred = str(top3.get("PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ", "")).strip()

    try:
        ens_gap = float(top3.get("PROèåˆæ¨¡å‹_gap", np.nan))
    except Exception:
        ens_gap = np.nan

    agree = (pro_pred != "") and (pro_pred == ens_pred)
    ens_is_away = (ens_pred == "å®¢èƒœ")
    ens_is_home = (ens_pred == "ä¸»èƒœ")

    # 1) Top3ï¼šèåˆå®¢èƒœ + ens_gap 0.05-0.1 + Parity=0ï¼ˆæ•´ä½“å¶ï¼‰
    if ens_is_away and _in_range(ens_gap, 0.05, 0.10) and overall_parity_bit == 0:
        msgs.append(f"è§„åˆ™1ï¼šTop3 èåˆ=å®¢èƒœ & èåˆgapâˆˆ[0.05,0.10] & total_countä¸ºå¶æ•°ï¼ˆtotal_count={overall_total_count}ï¼‰")

    # 2) Top3ï¼šèåˆå®¢èƒœ + ens_gap 0.15-0.2 + Parity=1ï¼ˆæ•´ä½“å¥‡ï¼‰
    if ens_is_away and _in_range(ens_gap, 0.15, 0.20) and overall_parity_bit == 1:
        msgs.append(f"è§„åˆ™2ï¼šTop3 èåˆ=å®¢èƒœ & èåˆgapâˆˆ[0.15,0.20] & total_countä¸ºå¥‡æ•°ï¼ˆtotal_count={overall_total_count}ï¼‰")

    # 3) Top3ï¼šèåˆå®¢èƒœ + ens_gap 0.05-0.1ï¼ˆä¸çœ‹Parityï¼‰
    if ens_is_away and _in_range(ens_gap, 0.05, 0.10):
        msgs.append("è§„åˆ™3ï¼šTop3 èåˆ=å®¢èƒœ & èåˆgapâˆˆ[0.05,0.10]ï¼ˆä¸çœ‹Parityï¼‰")

    # 4) Top3ï¼šèåˆä¸»èƒœ + Parity=3ï¼ˆæ•´ä½“ total_count==3ï¼‰
    if ens_is_home and (overall_total_count == 3):
        msgs.append("è§„åˆ™4ï¼šTop3 èåˆ=ä¸»èƒœ & total_count==3")

    # 5) Top3ï¼šä¸¤æ¨¡å‹ä¸€è‡´+é¢„æµ‹å®¢èƒœ+ens_gap 0.05-0.1
    if agree and ens_is_away and _in_range(ens_gap, 0.05, 0.10):
        msgs.append("è§„åˆ™5ï¼šTop3 ä¸¤æ¨¡å‹ä¸€è‡´=å®¢èƒœ & èåˆgapâˆˆ[0.05,0.10]")

    # 6) Top3ï¼šèåˆä¸»èƒœ+ens_gap 0-0.02 + total_count==3
    if ens_is_home and _in_range(ens_gap, 0.00, 0.02) and (overall_total_count == 3):
        msgs.append("è§„åˆ™6ï¼šTop3 èåˆ=ä¸»èƒœ & èåˆgapâˆˆ[0.00,0.02] & total_count==3")

    # 7) Top3ï¼šPRO=å¹³å±€ + èåˆ=ä¸»/å®¢ + ens_gap 0.02-0.05
    if (pro_pred == "å¹³å±€") and (ens_pred in ["ä¸»èƒœ", "å®¢èƒœ"]) and _in_range(ens_gap, 0.02, 0.05):
        msgs.append("è§„åˆ™7ï¼šTop3 PRO=å¹³å±€ & èåˆ=ä¸»èƒœ/å®¢èƒœ & èåˆgapâˆˆ[0.02,0.05]")

    # 8) Top3ï¼šä¸¤æ¨¡å‹ä¸€è‡´+é¢„æµ‹å®¢èƒœ+ens_gap 0.15-0.2
    if agree and ens_is_away and _in_range(ens_gap, 0.15, 0.20):
        msgs.append("è§„åˆ™8ï¼šTop3 ä¸¤æ¨¡å‹ä¸€è‡´=å®¢èƒœ & èåˆgapâˆˆ[0.15,0.20]")

    # 9) Parity=3ï¼šæ•´ä½“ total_count==3
    if overall_total_count == 3:
        msgs.append("è§„åˆ™9ï¼šæ•´ä½“ total_count==3ï¼ˆç‹¬ç«‹æç¤ºï¼‰")

    # å»é‡
    uniq = []
    for m in msgs:
        if m not in uniq:
            uniq.append(m)
    return uniq


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ é¡µé¢æ ‡é¢˜ & Session åˆå§‹åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("âš½ å†å²ç›¸ä¼¼æ¯”èµ› Top5 æŸ¥çœ‹ä¸å¯¼å‡ºï¼ˆParityå±•ç¤ºä¸º total_countï¼‰")

if "input_df" not in st.session_state:
    st.session_state.input_df = pd.DataFrame(columns=TEAM_COLS + ODDS_COLS)

if "matcher" not in st.session_state:
    if not HIST_PATH.exists():
        st.error(f"æ‰¾ä¸åˆ°å†å²åº“æ–‡ä»¶ï¼š{HIST_PATH}")
    else:
        st.session_state.matcher = SimilarityMatcher(str(HIST_PATH))

# ---------- æ•°æ®è¾“å…¥ ----------
mode = st.radio("ğŸ“¥ æ•°æ®è¾“å…¥æ–¹å¼", ["ä¸Šä¼ æ–‡ä»¶", "æ‰‹åŠ¨å½•å…¥"], horizontal=True)

if mode == "ä¸Šä¼ æ–‡ä»¶":
    up = st.file_uploader("ä¸Šä¼ èµ”ç‡æ–‡ä»¶ï¼ˆå»ºè®®å«ä¸»é˜Ÿ/å®¢é˜Ÿ + 15åˆ—èµ”ç‡ï¼‰", type=["xlsx", "csv"])
    if up is not None:
        df_up = pd.read_csv(up) if up.name.endswith(".csv") else pd.read_excel(up)
        for col in TEAM_COLS:
            if col not in df_up.columns:
                df_up[col] = ""
        missing_odds = [c for c in ODDS_COLS if c not in df_up.columns]
        if missing_odds:
            st.error(f"ä¸Šä¼ æ–‡ä»¶ç¼ºå°‘ä»¥ä¸‹èµ”ç‡åˆ—ï¼š{missing_odds}")
        else:
            st.session_state.input_df = df_up[TEAM_COLS + ODDS_COLS].copy()
            st.success(f"âœ… å·²è¯»å– {len(df_up)} åœºæ¯”èµ›")
            st.dataframe(st.session_state.input_df)

else:
    st.subheader("ğŸ–Š æ‰‹åŠ¨å½•å…¥ï¼ˆé€å…¬å¸ä¸€è¡Œï¼‰")
    with st.form("manual", clear_on_submit=True):
        c1, c2 = st.columns(2)
        home_team = c1.text_input("ä¸»é˜Ÿåç§°", key="home_team")
        away_team = c2.text_input("å®¢é˜Ÿåç§°", key="away_team")

        inps = {}
        st.markdown("è¯·è¾“å…¥å„åšå½©å…¬å¸èµ”ç‡ï¼ˆæ ¼å¼ï¼šä¸»èƒœ å¹³å±€ å®¢èƒœï¼Œä¾‹å¦‚ï¼š2.05 3.60 3.50ï¼‰")
        for comp in companies:
            r1, r2 = st.columns([1, 2])
            r1.markdown(f"<div class='company-name'>{comp}</div>", unsafe_allow_html=True)
            inps[comp] = r2.text_input("", placeholder="2.05 3.60 3.50", key=f"man_{comp}")

        if st.form_submit_button("æ·»åŠ æ¯”èµ›"):
            row_odds = []
            ok = True
            for comp in companies:
                parts = inps[comp].split()
                if len(parts) != 3:
                    st.error(f"{comp} éœ€è¾“å…¥ 3 ä¸ªèµ”ç‡")
                    ok = False
                    break
                try:
                    row_odds += [float(x) for x in parts]
                except ValueError:
                    st.error(f"{comp} çš„èµ”ç‡å¿…é¡»æ˜¯æ•°å­—")
                    ok = False
                    break

            if ok:
                new_row = pd.DataFrame([[home_team, away_team] + row_odds],
                                       columns=TEAM_COLS + ODDS_COLS)
                st.session_state.input_df = pd.concat(
                    [st.session_state.input_df, new_row],
                    ignore_index=True
                )
                st.success("âœ… å·²æ·»åŠ 1åœºæ¯”èµ›")
                st.dataframe(st.session_state.input_df)

# ---------- ä¸»é€»è¾‘ ----------
if not st.session_state.input_df.empty and "matcher" in st.session_state:
    st.subheader("ğŸ” å†å²ç›¸ä¼¼æ¯”èµ› Top5ï¼ˆå« total_count & Top3æç¤ºï¼‰")

    matcher: SimilarityMatcher = st.session_state.matcher
    df_odds = st.session_state.input_df[ODDS_COLS].copy()

    # å½“å‰æ¯”èµ› PRO æ¨¡å‹è¾“å‡º
    df_pro = predict_model_pro(df_odds)
    prob_cols = [c for c in df_pro.columns if c.startswith("P(")]
    for pc in prob_cols:
        df_pro[pc].fillna(0, inplace=True)

    # èåˆè¾“å‡º
    ens_in = pd.concat(
        [df_odds.reset_index(drop=True),
         df_pro[["average_gap"] + prob_cols].reset_index(drop=True)],
        axis=1
    )
    try:
        df_ens = predict_model_pro_ensemble(ens_in)
    except Exception:
        df_ens = pd.DataFrame({
            "PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ": ["å¹³å±€"] * len(df_pro),
            "PROèåˆæ¨¡å‹_gap": [0.0] * len(df_pro)
        })

    # METAï¼ˆç”¨äº q_basicï¼‰
    try:
        df_meta = predict_model_meta(df_odds)
    except Exception:
        df_meta = pd.DataFrame()

    # è¯»å–å‚è€ƒåº“æ˜ å°„ï¼ˆå«å½“å‰æ¯”èµ›ç»“æœï¼‰
    ref_map = load_ref_top5_map_with_result(REF_TOP5_PATH)
    if not ref_map:
        st.warning(f"âš ï¸ å‚è€ƒåº“æœªåŠ è½½æˆåŠŸæˆ–åˆ—åä¸åŒ¹é…ï¼š{REF_TOP5_PATH}ï¼ˆè‡³å°‘éœ€è¦ï¼šå½“å‰æ¯”èµ›åºå·ã€å†å²æ¯”èµ›åºå·ï¼›å¯é€‰ï¼šå½“å‰æ¯”èµ›ç»“æœï¼‰")

    export_rows = []

    for i in range(len(st.session_state.input_df)):
        home = st.session_state.input_df.loc[i, "ä¸»é˜Ÿ"] if "ä¸»é˜Ÿ" in st.session_state.input_df.columns else ""
        away = st.session_state.input_df.loc[i, "å®¢é˜Ÿ"] if "å®¢é˜Ÿ" in st.session_state.input_df.columns else ""

        title_str = f"ç¬¬ {i+1} åœº"
        if home or away:
            title_str += f"ï¼š{home} vs {away}"
        st.markdown(f"### â–¶ {title_str}")

        # èµ”ç‡è¿‘ä¼¼ç­‰å·®ä¸‰å…ƒç»„ï¼ˆä¸»/å¹³/å®¢ï¼Œå®¹å·®0.01ï¼‰
        input_row = st.session_state.input_df.loc[i]
        odds_ap = analyze_odds_ap_for_match(input_row, tolerance_ticks=1)

        st.markdown("**èµ”ç‡è¿‘ä¼¼ç­‰å·®é€’å¢ä¸‰å…ƒç»„ï¼ˆæˆªæ–­ä¸¤ä½å°æ•°ï¼Œä¸å››èˆäº”å…¥ï¼›å…è®¸å·®å€¼å·®â‰¤0.01ï¼‰**")
        st.caption(f"ä¸»èƒœ5èµ”ç‡(æˆªæ–­): {odds_ap['H_odds_trunc']} | å­˜åœ¨={odds_ap['H_odds_ap_has']} | ç»„æ•°={odds_ap['H_odds_ap_count']}")
        if odds_ap["H_odds_ap_triplets"]:
            st.caption("ä¸»èƒœä¸‰å…ƒç»„ï¼š " + odds_ap["H_odds_ap_triplets"])

        st.caption(f"å¹³å±€5èµ”ç‡(æˆªæ–­): {odds_ap['D_odds_trunc']} | å­˜åœ¨={odds_ap['D_odds_ap_has']} | ç»„æ•°={odds_ap['D_odds_ap_count']}")
        if odds_ap["D_odds_ap_triplets"]:
            st.caption("å¹³å±€ä¸‰å…ƒç»„ï¼š " + odds_ap["D_odds_ap_triplets"])

        st.caption(f"å®¢èƒœ5èµ”ç‡(æˆªæ–­): {odds_ap['A_odds_trunc']} | å­˜åœ¨={odds_ap['A_odds_ap_has']} | ç»„æ•°={odds_ap['A_odds_ap_count']}")
        if odds_ap["A_odds_ap_triplets"]:
            st.caption("å®¢èƒœä¸‰å…ƒç»„ï¼š " + odds_ap["A_odds_ap_triplets"])

        # å†å²ç›¸ä¼¼ Top5
        curr_pro_res = df_pro.loc[i, "æœ€ç»ˆé¢„æµ‹ç»“æœ"] if "æœ€ç»ˆé¢„æµ‹ç»“æœ" in df_pro.columns else ""
        curr_ens_res = df_ens.loc[i, "PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ"] if "PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ" in df_ens.columns else ""
        curr_pair = f"{curr_pro_res}-{curr_ens_res}" if curr_pro_res and curr_ens_res else ""

        q_basic = {
            "PRO_gap": df_pro.loc[i, "average_gap"],
            "PROèåˆæ¨¡å‹_gap": df_ens.loc[i, "PROèåˆæ¨¡å‹_gap"],
            "èåˆä¿¡å¿ƒ": df_meta.loc[i, "èåˆä¿¡å¿ƒ"] if "èåˆä¿¡å¿ƒ" in df_meta.columns else 0,
            "æ¨èæ€»åˆ†": df_meta.loc[i, "æ¨èæ€»åˆ†"] if "æ¨èæ€»åˆ†" in df_meta.columns else 0,
            "pair": curr_pair
        }

        try:
            sims_basic_full = matcher.query(q_basic, k=5)
        except Exception as e:
            st.warning(f"å†å²åŒ¹é…è°ƒç”¨å‡ºé”™ï¼š{e}")
            sims_basic_full = pd.DataFrame()

        sims_basic_full = sims_basic_full.reset_index(drop=True)
        if sims_basic_full.empty:
            st.info("æœªæ‰¾åˆ°å†å²ç›¸ä¼¼æ¯”èµ›ã€‚")
            continue

        # ===== å…ˆè®¡ç®—â€œæ•´ä½“ total_count/parity_bitâ€ï¼ˆæ¥è‡ªæ¨¡å¼è®¡æ•°ä½“ç³»ï¼‰=====
        # å…ˆä»Top5è®¡ç®—ä¸¤å¥— gap çš„å·®å€¼æ¨¡å¼
        pro_patterns = compute_gap_patterns(sims_basic_full, "PRO_gap")
        ens_patterns = compute_gap_patterns(sims_basic_full, "PROèåˆæ¨¡å‹_gap")
        pro0, pro1, pro2 = pro_patterns.get("0-1-2", ""), pro_patterns.get("1-2-3", ""), pro_patterns.get("2-3-4", "")
        ens0, ens1, ens2 = ens_patterns.get("0-1-2", ""), ens_patterns.get("1-2-3", ""), ens_patterns.get("2-3-4", "")

        counts = compute_pattern_counts_for_match(pro0, pro1, pro2, ens0, ens1, ens2)
        overall_total_count = counts["total_count"]
        overall_parity_bit = counts["parity_bit"]

        # å±•ç¤º Parityï¼ˆæŒ‰ä½ çš„è¦æ±‚ï¼šå±•ç¤ºæ€»æ¬¡æ•°ï¼Œä¸æ˜¾ç¤º0/1ï¼‰
        st.caption(f"**æ•´ä½“è®¡æ•°æ€»æ¬¡æ•° total_count = {overall_total_count}**ï¼ˆå¥‡å¶ç”¨äºè§„åˆ™åˆ¤æ–­ï¼š{'å¥‡' if overall_parity_bit==1 else 'å¶'}ï¼‰")

        # ===== Top3è§„åˆ™æç¤ºï¼šParityä½¿ç”¨æ•´ä½“total_count =====
        top3_msgs = check_top3_rules(
            sims_basic_full=sims_basic_full,
            overall_total_count=overall_total_count,
            overall_parity_bit=overall_parity_bit,
        )
        if top3_msgs:
            st.warning("âš ï¸ Top3 è§¦å‘è§„åˆ™æç¤ºï¼š\n\n- " + "\n- ".join(top3_msgs))

        # å½“å‰Top5åºåˆ—ï¼ˆæ¯”èµ›åºå· or æ¯”èµ›ç¼–å·ï¼‰
        id_col = "æ¯”èµ›åºå·" if "æ¯”èµ›åºå·" in sims_basic_full.columns else ("æ¯”èµ›ç¼–å·" if "æ¯”èµ›ç¼–å·" in sims_basic_full.columns else None)
        new_seq: List[int] = []
        if id_col is not None:
            new_seq = pd.to_numeric(sims_basic_full[id_col], errors="coerce").dropna().astype(int).tolist()[:5]

        # å‚è€ƒåº“åºåˆ—åŒ¹é…ï¼ˆâ‰¥4/5ï¼‰
        if len(new_seq) == 5 and ref_map:
            matches = match_top5_sequence(new_seq, ref_map)
            if matches:
                st.markdown("**ğŸ” å‚è€ƒåº“ Top5 åºåˆ—åŒ¹é…å‘½ä¸­ï¼ˆâ‰¥4/5 ä¸”é¡ºåºä¸€è‡´ï¼‰**")
                show_df = pd.DataFrame([{
                    "å‘½ä¸­ç­‰çº§": f"{m['level']}/5",
                    "å‚è€ƒåº“_å½“å‰æ¯”èµ›åºå·": m["ref_match_no"],
                    "å½“å‰æ¯”èµ›ç»“æœ": m.get("ref_result", ""),
                    "å‚è€ƒåº“_Top5åºåˆ—": "-".join(map(str, m["ref_seq"])),
                    "å‘½ä¸­çš„4åºåˆ—(è‹¥4/5)": "" if m["matched_subseq"] is None else "-".join(map(str, m["matched_subseq"])),
                } for m in matches])
                st.dataframe(show_df, use_container_width=True)
            else:
                st.caption("å‚è€ƒåº“åºåˆ—åŒ¹é…ï¼šæœªå‘½ä¸­ â‰¥4/5ã€‚")
        else:
            st.caption("å‚è€ƒåº“åºåˆ—åŒ¹é…ï¼šå½“å‰Top5åºåˆ—ä¸è¶³5ä¸ªæˆ–å‚è€ƒåº“æœªåŠ è½½ã€‚")

        # æ˜¾ç¤º Top5 è¡¨ï¼ˆæ–°å¢ gap_sum_100 åˆ—ï¼‰
        sims_show = sims_basic_full.copy()

        if "æ¯”èµ›åºå·" not in sims_show.columns:
            if "æ¯”èµ›ç¼–å·" in sims_show.columns:
                sims_show["æ¯”èµ›åºå·"] = sims_show["æ¯”èµ›ç¼–å·"]
            else:
                sims_show["æ¯”èµ›åºå·"] = ""

        if "æ¯”èµ›ç»“æœ" not in sims_show.columns:
            if "æ¯”èµ›ç»“æœ_y" in sims_show.columns:
                sims_show["æ¯”èµ›ç»“æœ"] = sims_show["æ¯”èµ›ç»“æœ_y"]
            elif "æ¯”èµ›ç»“æœ_x" in sims_show.columns:
                sims_show["æ¯”èµ›ç»“æœ"] = sims_show["æ¯”èµ›ç»“æœ_x"]
            else:
                sims_show["æ¯”èµ›ç»“æœ"] = ""

        for col in ["PRO_æœ€ç»ˆé¢„æµ‹ç»“æœ", "PRO_gap", "PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ", "PROèåˆæ¨¡å‹_gap"]:
            if col not in sims_show.columns:
                sims_show[col] = ""

        sims_show["gap_sum_100"] = sims_show.apply(
            lambda r: compute_gap_sum_100(r.get("PRO_gap", 0.0), r.get("PROèåˆæ¨¡å‹_gap", 0.0)),
            axis=1
        )

        # ä½ è¦å±•ç¤º total_countï¼ˆæ•´ä½“ï¼‰ï¼Œè¿™é‡ŒåŠ ä¸€åˆ—ç»™Top5æ¯è¡Œéƒ½åŒæ ·å€¼ï¼ˆä¾¿äºå¯¼å‡º/æŸ¥çœ‹ï¼‰
        sims_show["total_count"] = overall_total_count

        sims_show = sims_show[[
            "æ¯”èµ›åºå·", "æ¯”èµ›ç»“æœ",
            "PRO_æœ€ç»ˆé¢„æµ‹ç»“æœ", "PRO_gap",
            "PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ", "PROèåˆæ¨¡å‹_gap",
            "gap_sum_100",
            "total_count"
        ]]

        sims_show["PRO_gap"] = pd.to_numeric(sims_show["PRO_gap"], errors="coerce").round(4)
        sims_show["PROèåˆæ¨¡å‹_gap"] = pd.to_numeric(sims_show["PROèåˆæ¨¡å‹_gap"], errors="coerce").round(4)

        render_compact_table(sims_show)

        # ===== å¯¼å‡ºTop5ï¼ˆCSVï¼Œåˆ†ç»„ç©ºè¡Œï¼‰=====
        hist_home_col = "ä¸»é˜Ÿ" if "ä¸»é˜Ÿ" in sims_basic_full.columns else None
        hist_away_col = "å®¢é˜Ÿ" if "å®¢é˜Ÿ" in sims_basic_full.columns else None

        for _, r in sims_basic_full.iterrows():
            hist_pro_gap = r.get("PRO_gap", 0.0)
            hist_ens_gap = r.get("PROèåˆæ¨¡å‹_gap", 0.0)
            export_rows.append({
                "å½“å‰æ¯”èµ›åºå·": i + 1,
                "å½“å‰ä¸»é˜Ÿ": home,
                "å½“å‰å®¢é˜Ÿ": away,

                "å†å²æ¯”èµ›åºå·": r.get("æ¯”èµ›åºå·", r.get("æ¯”èµ›ç¼–å·", "")),
                "å†å²æ¯”èµ›ç»“æœ": get_result_value(r),
                "å†å²PRO_æœ€ç»ˆé¢„æµ‹ç»“æœ": r.get("PRO_æœ€ç»ˆé¢„æµ‹ç»“æœ", ""),
                "å†å²PRO_gap": hist_pro_gap,
                "å†å²PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ": r.get("PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ", ""),
                "å†å²PROèåˆæ¨¡å‹_gap": hist_ens_gap,
                "å†å²gap_sum_100": compute_gap_sum_100(hist_pro_gap, hist_ens_gap),

                # æ–°å¢ï¼šæ•´ä½“ total_countï¼ˆä½œä¸ºä½ è¦å±•ç¤ºçš„â€œParityæ€»æ¬¡æ•°â€ï¼‰
                "total_count": overall_total_count,

                "å†å²ä¸»é˜Ÿ": r.get(hist_home_col, "") if hist_home_col else "",
                "å†å²å®¢é˜Ÿ": r.get(hist_away_col, "") if hist_away_col else "",
            })

    # ===== å¯¼å‡ºï¼ˆæ’å…¥ç©ºè¡Œåˆ†ç»„ï¼‰=====
    if export_rows:
        df_export = pd.DataFrame(export_rows)

        for col in ["å†å²PRO_gap", "å†å²PROèåˆæ¨¡å‹_gap"]:
            if col in df_export.columns:
                df_export[col] = pd.to_numeric(df_export[col], errors="coerce").round(4)

        rows_with_blank = []
        last_match = None
        for _, row in df_export.iterrows():
            match_no = row["å½“å‰æ¯”èµ›åºå·"]
            if last_match is not None and match_no != last_match:
                rows_with_blank.append({col: "" for col in df_export.columns})
            rows_with_blank.append(row.to_dict())
            last_match = match_no

        df_export_with_blank = pd.DataFrame(rows_with_blank, columns=df_export.columns)

        st.subheader("ğŸ“¤ å¯¼å‡ºæ‰€æœ‰æ¯”èµ›çš„å†å²ç›¸ä¼¼ Top5ï¼ˆCSVï¼Œåˆ†ç»„ç©ºè¡Œï¼‰")
        render_compact_table(df_export_with_blank.head(20))

        st.download_button(
            "â¬‡ï¸ å¯¼å‡ºå†å²ç›¸ä¼¼ Top5ï¼ˆCSVï¼‰",
            df_export_with_blank.to_csv(index=False).encode("utf-8-sig"),
            "all_matches_top5_history_export.csv",
            "text/csv",
        )
