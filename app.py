# app.py  â€“  å†å²ç›¸ä¼¼æ¯”èµ› Top5 æŸ¥çœ‹ä¸å¯¼å‡º
# + å†å² TOP5 çš„ 6 ä¸ªå·®å€¼æ¨¡å¼
# + æ¨¡å¼è®¡æ•°ä½“ç³»ï¼ˆequal_pro/diff1_pro/...ï¼‰
# + PRO_gap & PROèåˆæ¨¡å‹_gap ç­‰å·®é€’å¢ä¸‰å…ƒç»„æ£€æµ‹ï¼ˆæˆªæ–­ä¸¤ä½ï¼Œä¸å››èˆäº”å…¥ï¼‰
# + 5å®¶å…¬å¸ä¸»/å¹³/å®¢èµ”ç‡ï¼šè¿‘ä¼¼ç­‰å·®é€’å¢ä¸‰å…ƒç»„ï¼ˆæˆªæ–­ä¸¤ä½ï¼Œä¸å››èˆäº”å…¥ï¼›å…è®¸å·®å€¼å·®<=0.01ï¼›åˆ—å‡ºå…·ä½“ä¸‰å…ƒç»„ï¼‰
# + æ–°å¢ï¼šTop5 æ¯è¡Œè®¡ç®— gap_sum_100 = floor(PRO_gap*100) + floor(PROèåˆæ¨¡å‹_gap*100)
# + æ–°å¢ï¼šå°†â€œæœ¬æ¬¡Top5åºåˆ—â€å»åŒ¹é…199åº“Top5åºåˆ—ï¼Œå‘½ä¸­>=4/5åˆ™æ˜¾ç¤ºï¼ˆé¡ºåºå¿…é¡»ä¸€è‡´ï¼‰

import math
import re
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any

import numpy as np
import pandas as pd
import streamlit as st

from models.pro_model import predict_model_pro
from models.model_pro_ensemble import predict_model_pro_ensemble
from models.predict_model_meta import predict_model_meta
from similarity_matcher import SimilarityMatcher

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI è®¾ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="å†å²ç›¸ä¼¼æ¯”èµ› Top5 å¯¼å‡º", layout="wide")

st.markdown("""
<style>
.stApp { background:linear-gradient(135deg,#f0f8ff,#e6e6fa); }
.company-name{font-size:1.05em;font-weight:600;text-shadow:1px 1px 2px rgba(0,0,0,0.2);}
.stTextInput>div>div>input{max-width:260px;}
.stButton>button{margin-top:4px;margin-bottom:8px;}

/* ç´§å‡‘å†å²è¡¨æ ¼æ ·å¼ */
.hist-table{
  table-layout: fixed;
  width: 100%;
  border-collapse: collapse;
}
.hist-table th, .hist-table td{
  max-width: 80px;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
  padding: 3px 4px;
  border-bottom: 1px solid #ddd;
  font-size: 0.85rem;
}
.hist-table th{
  font-weight: 600;
  background-color:#f5f5ff;
}

/* å·®å€¼æ¨¡å¼çš„å°è¡¨æ ¼ */
.pattern-table{
  border-collapse: collapse;
  margin-top: 4px;
}
.pattern-table th, .pattern-table td{
  border: 1px solid #ddd;
  padding: 3px 6px;
  font-size: 0.85rem;
}
.pattern-table th{
  background-color:#eef2ff;
  font-weight: 600;
}
</style>
""", unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ åŸºæœ¬å¸¸é‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR  = Path(__file__).parent
DATA_FILE = BASE_DIR / "data" / "new_matches.xlsx"
HIST_PATH = BASE_DIR / "data" / "prediction_results (43).xlsx"

# 199åº“Top5å‚è€ƒæ–‡ä»¶ï¼ˆç›¸å¯¹è·¯å¾„ï¼Œæ–¹ä¾¿GitHubéƒ¨ç½²ï¼‰
REF_TOP5_PATH = BASE_DIR / "data" / "all_matches_top5_history.csv"

companies = ["Bet365", "ç«‹åš", "Interwetten", "Pinnacle", "William Hill"]
TEAM_COLS = ["ä¸»é˜Ÿ", "å®¢é˜Ÿ"]
outcomes = ["ä¸»èƒœ", "å¹³å±€", "å®¢èƒœ"]
ODDS_COLS = [f"{c}_{o}" for c in companies for o in outcomes]
OUTCOME_COL = "æ¯”èµ›ç»“æœ"

# ---------- æ¸²æŸ“ç´§å‡‘è¡¨æ ¼ ----------
def render_compact_table(df: pd.DataFrame):
    """ç”¨å›ºå®šåˆ—å®½çš„ HTML è¡¨æ ¼æ˜¾ç¤º DataFrameï¼Œå¹¶ç»Ÿä¸€æ•°å€¼ä¸º 4 ä½å°æ•°ã€‚"""
    if df is None or df.empty:
        st.info("æš‚æ— æ•°æ®ã€‚")
        return
    html = df.to_html(
        index=False,
        classes="hist-table",
        border=0,
        escape=False,
        float_format=lambda x: f"{x:.4f}",
    )
    st.markdown(html, unsafe_allow_html=True)

# ---------- è¯†åˆ«â€œæ¯”èµ›ç»“æœâ€åˆ—å ----------
def get_result_value(row: pd.Series) -> str:
    """ä»ä¸€è¡Œä¸­å–æ¯”èµ›ç»“æœï¼šä¼˜å…ˆæ¯”èµ›ç»“æœï¼Œå…¶æ¬¡æ¯”èµ›ç»“æœ_yï¼Œå†å…¶æ¬¡æ¯”èµ›ç»“æœ_xã€‚"""
    for col in ["æ¯”èµ›ç»“æœ", "æ¯”èµ›ç»“æœ_y", "æ¯”èµ›ç»“æœ_x"]:
        if col in row and pd.notna(row[col]) and str(row[col]) != "":
            return str(row[col])
    return ""

# ---------- æˆªæ–­å·¥å…·ï¼šä¸¤ä½å°æ•°ï¼Œä¸å››èˆäº”å…¥ ----------
def _truncate_two_decimals(x: float) -> float:
    """æˆªæ–­åˆ°ä¸¤ä½å°æ•°ï¼ˆä¸å››èˆäº”å…¥ï¼‰ã€‚"""
    try:
        x = float(x)
    except (TypeError, ValueError):
        return 0.0
    if x >= 0:
        return math.floor(x * 100 + 1e-8) / 100.0
    else:
        return math.ceil(x * 100 - 1e-8) / 100.0

def _floor_times_100(x: float) -> int:
    """ç­‰ä»·äºï¼šå…ˆæˆªæ–­åˆ°ä¸¤ä½å°æ•°å†Ã—100ï¼ˆå¯¹æ­£æ•°å°±æ˜¯ floor(x*100)ï¼‰ã€‚"""
    try:
        x = float(x)
    except (TypeError, ValueError):
        return 0
    if math.isnan(x):
        return 0
    return int(math.floor(x * 100 + 1e-8))

def compute_gap_sum_100(pro_gap: float, ens_gap: float) -> int:
    """
    1) å„å–ä¸¤ä½å°æ•°ï¼ˆä¸å››èˆäº”å…¥ï¼‰ç­‰ä»·äº floor(x*100)/100
    2) å†Ã—100ç­‰ä»·äº floor(x*100)
    3) ä¸¤è€…ç›¸åŠ 
    """
    return _floor_times_100(pro_gap) + _floor_times_100(ens_gap)

# ---------- å·®å€¼æ¨¡å¼å·¥å…· ----------
def format_gap_pattern(values) -> str:
    """3 ä¸ª gap â†’ '(outer)d1-d2' æ¨¡å¼å­—ç¬¦ä¸²ã€‚"""
    if len(values) != 3:
        return ""
    vs = sorted(_truncate_two_decimals(v) for v in values)
    d1 = int(math.floor(abs(vs[1] - vs[0]) * 100 + 1e-8))
    d2 = int(math.floor(abs(vs[2] - vs[1]) * 100 + 1e-8))
    if (d1 == 0 and d2 != 0) or (d2 == 0 and d1 != 0):
        first, second = 0, d2 if d1 == 0 else d1
    else:
        first, second = d1, d2
    outer = abs(first - second)
    return f"({outer}){first}-{second}"

def compute_gap_patterns(sims: pd.DataFrame, col: str) -> dict:
    """
    ä¸ºæŒ‡å®š gap åˆ—ï¼ˆå¦‚ 'PRO_gap' æˆ– 'PROèåˆæ¨¡å‹_gap'ï¼‰è®¡ç®—ï¼š
      - 0-1-2, 1-2-3, 2-3-4 ä¸‰ä¸ªçª—å£çš„å·®å€¼æ¨¡å¼ã€‚
    """
    if col not in sims.columns:
        return {}
    vals = sims[col].tolist()
    patterns = {}
    for label, start in [("0-1-2", 0), ("1-2-3", 1), ("2-3-4", 2)]:
        if len(vals) >= start + 3:
            patterns[label] = format_gap_pattern(vals[start:start+3])
        else:
            patterns[label] = ""
    return patterns

# ---------- PRO_gap & ENS_gap ç­‰å·®ä¸‰å…ƒç»„å·¥å…· ----------
def compute_gap_ap(sims: pd.DataFrame, col: str):
    """
    å¯¹å½“å‰æ¯”èµ›çš„ Top5 æŸåˆ— gapï¼ˆå¦‚ PRO_gap æˆ– PROèåˆæ¨¡å‹_gapï¼‰ï¼š
      - æˆªæ–­åˆ°ä¸¤ä½å°æ•°ï¼ˆä¸å››èˆäº”å…¥ï¼‰ï¼›
      - æ‰¾ä¸¥æ ¼é€’å¢ç­‰å·®ä¸‰å…ƒç»„ï¼›
      - best_triplet é€‰å…¬å·®æœ€å°çš„é‚£ç»„ï¼›
    """
    if col not in sims.columns or sims.empty:
        return [], 0, None, []

    gaps = sims[col].tolist()
    trunc = [_truncate_two_decimals(x) for x in gaps]

    ints = [int(round(v * 100)) for v in trunc]
    uniq = sorted(set(ints))

    has_ap = 0
    best_triplet = None
    best_step = None
    all_triplets: List[Tuple[float, float, float]] = []

    n = len(uniq)
    for i in range(n):
        for j in range(i + 1, n):
            for k in range(j + 1, n):
                a, b, c = uniq[i], uniq[j], uniq[k]
                if b > a and c > b and (b - a) == (c - b):
                    has_ap = 1
                    step = b - a
                    triplet = (a / 100.0, b / 100.0, c / 100.0)
                    all_triplets.append(triplet)
                    if best_step is None or step < best_step:
                        best_step = step
                        best_triplet = triplet

    return trunc, has_ap, best_triplet, all_triplets

# ---------- æ¨¡å¼è§£æ & è®¡æ•°ä½“ç³» ----------
def parse_pair_from_pattern(pat: str) -> Optional[Tuple[int, int]]:
    if not isinstance(pat, str) or not pat.strip():
        return None
    nums = re.findall(r"\d+", pat)
    if len(nums) < 2:
        return None
    a, b = int(nums[-2]), int(nums[-1])
    return a, b

def delta_from_pattern(pat: str) -> Optional[int]:
    pair = parse_pair_from_pattern(pat)
    if pair is None:
        return None
    a, b = pair
    return abs(b - a)

def compute_pattern_counts_for_match(
    pro0: str, pro1: str, pro2: str,
    ens0: str, ens1: str, ens2: str,
):
    pro_pats = [pro0, pro1, pro2]
    ens_pats = [ens0, ens1, ens2]

    pro_pairs = [parse_pair_from_pattern(p) for p in pro_pats]
    pro_deltas = [delta_from_pattern(p) for p in pro_pats]

    ens_pairs = [parse_pair_from_pattern(p) for p in ens_pats]
    ens_deltas = [delta_from_pattern(p) for p in ens_pats]

    equal_pro = 0
    diff1_pro = 0
    for i in range(3):
        for j in range(i + 1, 3):
            di, dj = pro_deltas[i], pro_deltas[j]
            pi, pj = pro_pairs[i], pro_pairs[j]
            if di is None or dj is None or pi is None or pj is None:
                continue
            if di == dj:
                equal_pro += 1
            if abs(di - dj) == 1:
                if set(pi) & set(pj):
                    diff1_pro += 1

    equal_ens = 0
    diff1_ens = 0
    for i in range(3):
        for j in range(i + 1, 3):
            di, dj = ens_deltas[i], ens_deltas[j]
            pi, pj = ens_pairs[i], ens_pairs[j]
            if di is None or dj is None or pi is None or pj is None:
                continue
            if di == dj:
                equal_ens += 1
            if abs(di - dj) == 1:
                if set(pi) & set(pj):
                    diff1_ens += 1

    equal_cross = 0
    diff1_cross = 0
    for i in range(3):
        for j in range(3):
            di, dj = pro_deltas[i], ens_deltas[j]
            pi, pj = pro_pairs[i], ens_pairs[j]
            if di is None or dj is None or pi is None or pj is None:
                continue
            if di == dj:
                equal_cross += 1
            if abs(di - dj) == 1:
                if not (set(pi) & set(pj)):
                    diff1_cross += 1

    total = equal_pro + diff1_pro + equal_ens + diff1_ens + equal_cross + diff1_cross
    parity = 1 if total % 2 == 1 else 0

    return {
        "equal_pro": equal_pro,
        "diff1_pro": diff1_pro,
        "equal_ens": equal_ens,
        "diff1_ens": diff1_ens,
        "equal_cross": equal_cross,
        "diff1_cross": diff1_cross,
        "total_count": total,
        "parity": parity,
    }

# ========== èµ”ç‡ï¼ˆä¸‰å…ƒç»„è¿‘ä¼¼ç­‰å·®ï¼‰å·¥å…· ==========
def find_ap_triplets_for_odds(company_odds: List[Tuple[str, float]], tolerance_ticks: int = 1):
    """
    è§„åˆ™ï¼ˆè¿‘ä¼¼ç­‰å·®ï¼‰ï¼š
      - èµ”ç‡æˆªæ–­ä¸¤ä½å°æ•°ï¼ˆä¸å››èˆäº”å…¥ï¼‰
      - ä¸‰å…ƒç»„æŒ‰èµ”ç‡ä»å°åˆ°å¤§
      - tick=0.01ï¼ˆä¸¤ä½å°æ•°*100ï¼‰
      - |d1-d2|<=1 åˆ™è¿‘ä¼¼ç­‰å·®
      - ä¸¥æ ¼é€’å¢
    """
    trunc_list = [(c, _truncate_two_decimals(v)) for c, v in company_odds]

    triplets_desc: List[str] = []
    for a in range(5):
        for b in range(a + 1, 5):
            for c in range(b + 1, 5):
                t = [trunc_list[a], trunc_list[b], trunc_list[c]]
                t_sorted = sorted(t, key=lambda x: x[1])

                v1, v2, v3 = t_sorted[0][1], t_sorted[1][1], t_sorted[2][1]
                if not (v1 < v2 < v3):
                    continue

                i1, i2, i3 = int(round(v1 * 100)), int(round(v2 * 100)), int(round(v3 * 100))
                d1_ticks = i2 - i1
                d2_ticks = i3 - i2

                if abs(d1_ticks - d2_ticks) <= tolerance_ticks:
                    d1 = d1_ticks / 100.0
                    d2 = d2_ticks / 100.0
                    delta = abs(d1_ticks - d2_ticks) / 100.0
                    triplets_desc.append(
                        f"{t_sorted[0][0]}:{v1:.2f} < {t_sorted[1][0]}:{v2:.2f} < {t_sorted[2][0]}:{v3:.2f}"
                        f" | d1={d1:.2f} d2={d2:.2f} |Î”|={delta:.2f}"
                    )

    trunc_vals = [v for _, v in trunc_list]
    return trunc_vals, triplets_desc

def analyze_odds_ap_for_match(input_row: pd.Series, tolerance_ticks: int = 1):
    result = {}
    for outcome in ["ä¸»èƒœ", "å¹³å±€", "å®¢èƒœ"]:
        company_odds = []
        for comp in companies:
            col = f"{comp}_{outcome}"
            company_odds.append((comp, float(input_row[col]) if pd.notna(input_row[col]) else 0.0))

        trunc_vals, trips = find_ap_triplets_for_odds(company_odds, tolerance_ticks=tolerance_ticks)
        key_prefix = {"ä¸»èƒœ": "H", "å¹³å±€": "D", "å®¢èƒœ": "A"}[outcome]

        result[f"{key_prefix}_odds_trunc"] = ",".join(f"{v:.2f}" for v in trunc_vals)
        result[f"{key_prefix}_odds_ap_has"] = 1 if trips else 0
        result[f"{key_prefix}_odds_ap_count"] = len(trips)
        result[f"{key_prefix}_odds_ap_triplets"] = "ï¼› ".join(trips)

    return result

# ========== æ–°å¢ï¼š199åº“Top5åºåˆ—åŒ¹é… ==========
def _detect_col(df: pd.DataFrame, candidates: List[str]) -> str:
    for c in candidates:
        if c in df.columns:
            return c
    return ""

@st.cache_data
def load_ref_top5_map(ref_path: Path) -> Dict[int, List[int]]:
    """
    è¯»å–199åº“Top5 CSVï¼Œç”Ÿæˆï¼š
      { å½“å‰æ¯”èµ›åºå·(int): [å†å²æ¯”èµ›åºå·(int) * 5] }
    å…¼å®¹åˆ—åï¼šå½“å‰æ¯”èµ›åºå·/å†å²æ¯”èµ›åºå·ï¼ˆæˆ–æ¯”èµ›ç¼–å·/æ¯”èµ›åºå·ç­‰ï¼‰
    """
    if not ref_path.exists():
        return {}

    df = pd.read_csv(ref_path)

    cur_col = _detect_col(df, ["å½“å‰æ¯”èµ›åºå·", "å½“å‰æ¯”èµ›ç¼–å·", "å½“å‰æ¯”èµ›ID", "å½“å‰åºå·"])
    hist_col = _detect_col(df, ["å†å²æ¯”èµ›åºå·", "å†å²æ¯”èµ›ç¼–å·", "å†å²æ¯”èµ›ID", "å†å²åºå·"])

    # å…œåº•ï¼šå¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ç”¨â€œå†å²æ¯”èµ›åºå·â€å¸¸è§å¯¼å‡ºå­—æ®µ
    if cur_col == "" or hist_col == "":
        # å†å…œåº•ä¸€æ¬¡ï¼šæœ‰äº›å¯¼å‡ºå¯èƒ½å«â€œå½“å‰æ¯”èµ›åºå·â€â€œå†å²æ¯”èµ›åºå·â€ï¼Œè¿™é‡Œç›´æ¥æŠ¥é”™æç¤º
        return {}

    df = df.dropna(subset=[cur_col, hist_col]).copy()
    df[cur_col] = pd.to_numeric(df[cur_col], errors="coerce")
    df[hist_col] = pd.to_numeric(df[hist_col], errors="coerce")
    df = df.dropna(subset=[cur_col, hist_col]).copy()
    df[cur_col] = df[cur_col].astype(int)
    df[hist_col] = df[hist_col].astype(int)

    ref_map: Dict[int, List[int]] = {}
    # ç”¨è¡Œé¡ºåºç´¯ç§¯ï¼Œä¿è¯â€œå‰åé¡ºåºä¸€è‡´â€
    for _, r in df.iterrows():
        k = int(r[cur_col])
        v = int(r[hist_col])
        ref_map.setdefault(k, []).append(v)

    # åªä¿ç•™å‰5
    for k in list(ref_map.keys()):
        if len(ref_map[k]) >= 5:
            ref_map[k] = ref_map[k][:5]
        else:
            # ä¸è¶³5çš„ä¸å‚ä¸åŒ¹é…
            ref_map.pop(k, None)

    return ref_map

def match_top5_sequence(new_seq: List[int], ref_map: Dict[int, List[int]]) -> List[Dict[str, Any]]:
    """
    new_seq é•¿åº¦=5ã€‚åŒ¹é…è§„åˆ™ï¼š
      - 5/5ï¼šå®Œå…¨ä¸€è‡´
      - 4/5ï¼šåˆ æ‰1ä¸ªå…ƒç´ åï¼Œå­˜åœ¨4é•¿å­åºåˆ—å®Œå…¨ä¸€è‡´ï¼ˆé¡ºåºä¸€è‡´ï¼‰
    è¿”å›åŒ¹é…åˆ—è¡¨ï¼ˆlevel=5æˆ–4ï¼‰
    """
    results: List[Dict[str, Any]] = []
    if len(new_seq) != 5 or not ref_map:
        return results

    new_tuple = tuple(new_seq)
    new_sub4 = [tuple([new_seq[j] for j in range(5) if j != drop_i]) for drop_i in range(5)]

    for ref_match_no, ref_seq in ref_map.items():
        if len(ref_seq) != 5:
            continue
        ref_tuple = tuple(ref_seq)

        if ref_tuple == new_tuple:
            results.append({
                "ref_match_no": ref_match_no,
                "level": 5,
                "ref_seq": ref_seq,
                "matched_subseq": None,
            })
            continue

        ref_sub4_set = set(tuple([ref_seq[k] for k in range(5) if k != drop_j]) for drop_j in range(5))
        hit_sub = None
        for sub in new_sub4:
            if sub in ref_sub4_set:
                hit_sub = list(sub)
                break

        if hit_sub is not None:
            results.append({
                "ref_match_no": ref_match_no,
                "level": 4,
                "ref_seq": ref_seq,
                "matched_subseq": hit_sub,
            })

    results.sort(key=lambda x: (-x["level"], x["ref_match_no"]))
    return results

# â”€â”€â”€â”€â”€â”€â”€â”€â”€ é¡µé¢æ ‡é¢˜ & Session åˆå§‹åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("âš½ å†å²ç›¸ä¼¼æ¯”èµ› Top5 æŸ¥çœ‹ä¸å¯¼å‡ºï¼ˆå« gap_sum_100 + 199åº“åºåˆ—åŒ¹é…ï¼‰")

if "input_df" not in st.session_state:
    st.session_state.input_df = pd.DataFrame(columns=TEAM_COLS + ODDS_COLS)

if "matcher" not in st.session_state:
    if not HIST_PATH.exists():
        st.error(f"æ‰¾ä¸åˆ°å†å²åº“æ–‡ä»¶ï¼š{HIST_PATH}")
    else:
        st.session_state.matcher = SimilarityMatcher(str(HIST_PATH))

# ---------- æ•°æ®è¾“å…¥ ----------
mode = st.radio("ğŸ“¥ æ•°æ®è¾“å…¥æ–¹å¼", ["ä¸Šä¼ æ–‡ä»¶", "æ‰‹åŠ¨å½•å…¥"], horizontal=True)

if mode == "ä¸Šä¼ æ–‡ä»¶":
    up = st.file_uploader("ä¸Šä¼ èµ”ç‡æ–‡ä»¶ï¼ˆå»ºè®®å«ä¸»é˜Ÿ/å®¢é˜Ÿ + 15åˆ—èµ”ç‡ï¼‰", type=["xlsx", "csv"])
    if up is not None:
        df_up = pd.read_csv(up) if up.name.endswith(".csv") else pd.read_excel(up)
        for col in TEAM_COLS:
            if col not in df_up.columns:
                df_up[col] = ""
        missing_odds = [c for c in ODDS_COLS if c not in df_up.columns]
        if missing_odds:
            st.error(f"ä¸Šä¼ æ–‡ä»¶ç¼ºå°‘ä»¥ä¸‹èµ”ç‡åˆ—ï¼š{missing_odds}")
        else:
            st.session_state.input_df = df_up[TEAM_COLS + ODDS_COLS].copy()
            st.success(f"âœ… å·²è¯»å– {len(df_up)} åœºæ¯”èµ›")
            st.dataframe(st.session_state.input_df)

else:
    st.subheader("ğŸ–Š æ‰‹åŠ¨å½•å…¥ï¼ˆé€å…¬å¸ä¸€è¡Œï¼‰")
    with st.form("manual", clear_on_submit=True):
        c1, c2 = st.columns(2)
        home_team = c1.text_input("ä¸»é˜Ÿåç§°", key="home_team")
        away_team = c2.text_input("å®¢é˜Ÿåç§°", key="away_team")

        inps = {}
        st.markdown("è¯·è¾“å…¥å„åšå½©å…¬å¸èµ”ç‡ï¼ˆæ ¼å¼ï¼šä¸»èƒœ å¹³å±€ å®¢èƒœï¼Œä¾‹å¦‚ï¼š2.05 3.60 3.50ï¼‰")
        for comp in companies:
            r1, r2 = st.columns([1, 2])
            r1.markdown(f"<div class='company-name'>{comp}</div>", unsafe_allow_html=True)
            inps[comp] = r2.text_input("", placeholder="2.05 3.60 3.50", key=f"man_{comp}")

        if st.form_submit_button("æ·»åŠ æ¯”èµ›"):
            row_odds = []
            ok = True
            for comp in companies:
                parts = inps[comp].split()
                if len(parts) != 3:
                    st.error(f"{comp} éœ€è¾“å…¥ 3 ä¸ªèµ”ç‡")
                    ok = False
                    break
                try:
                    row_odds += [float(x) for x in parts]
                except ValueError:
                    st.error(f"{comp} çš„èµ”ç‡å¿…é¡»æ˜¯æ•°å­—")
                    ok = False
                    break

            if ok:
                new_row = pd.DataFrame([[home_team, away_team] + row_odds],
                                       columns=TEAM_COLS + ODDS_COLS)
                st.session_state.input_df = pd.concat(
                    [st.session_state.input_df, new_row],
                    ignore_index=True
                )
                st.success("âœ… å·²æ·»åŠ 1åœºæ¯”èµ›")
                st.dataframe(st.session_state.input_df)

# ---------- ä¸»é€»è¾‘ ----------
if not st.session_state.input_df.empty and "matcher" in st.session_state:
    st.subheader("ğŸ” å†å²ç›¸ä¼¼æ¯”èµ› Top5ï¼ˆå« gap_sum_100 + 199åº“åºåˆ—åŒ¹é…ï¼‰")

    matcher: SimilarityMatcher = st.session_state.matcher
    df_odds = st.session_state.input_df[ODDS_COLS].copy()

    # å½“å‰æ¯”èµ› PRO æ¨¡å‹è¾“å‡º
    df_pro = predict_model_pro(df_odds)
    prob_cols = [c for c in df_pro.columns if c.startswith("P(")]
    for pc in prob_cols:
        df_pro[pc].fillna(0, inplace=True)

    # èåˆè¾“å‡º
    ens_in = pd.concat(
        [df_odds.reset_index(drop=True),
         df_pro[["average_gap"] + prob_cols].reset_index(drop=True)],
        axis=1
    )
    try:
        df_ens = predict_model_pro_ensemble(ens_in)
    except Exception:
        df_ens = pd.DataFrame({
            "PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ": ["å¹³å±€"] * len(df_pro),
            "PROèåˆæ¨¡å‹_gap": [0.0] * len(df_pro)
        })

    # METAï¼ˆç”¨äº q_basicï¼‰
    try:
        df_meta = predict_model_meta(df_odds)
    except Exception:
        df_meta = pd.DataFrame()

    # è¯»å–199åº“æ˜ å°„ï¼ˆç”¨äºåºåˆ—åŒ¹é…ï¼‰
    ref_map_199 = load_ref_top5_map(REF_TOP5_PATH)
    if not ref_map_199:
        st.warning(f"âš ï¸ 199åº“Top5æ–‡ä»¶æœªåŠ è½½æˆåŠŸæˆ–åˆ—åä¸åŒ¹é…ï¼š{REF_TOP5_PATH}ï¼ˆéœ€è¦åŒ…å«ï¼šå½“å‰æ¯”èµ›åºå·ã€å†å²æ¯”èµ›åºå·ï¼‰")

    export_rows = []

    for i in range(len(st.session_state.input_df)):
        home = st.session_state.input_df.loc[i, "ä¸»é˜Ÿ"] if "ä¸»é˜Ÿ" in st.session_state.input_df.columns else ""
        away = st.session_state.input_df.loc[i, "å®¢é˜Ÿ"] if "å®¢é˜Ÿ" in st.session_state.input_df.columns else ""

        title_str = f"ç¬¬ {i+1} åœº"
        if home or away:
            title_str += f"ï¼š{home} vs {away}"
        st.markdown(f"### â–¶ {title_str}")

        # èµ”ç‡è¿‘ä¼¼ç­‰å·®ä¸‰å…ƒç»„ï¼ˆä¸»/å¹³/å®¢ï¼Œå®¹å·®0.01ï¼‰
        input_row = st.session_state.input_df.loc[i]
        odds_ap = analyze_odds_ap_for_match(input_row, tolerance_ticks=1)

        st.markdown("**èµ”ç‡è¿‘ä¼¼ç­‰å·®é€’å¢ä¸‰å…ƒç»„ï¼ˆæˆªæ–­ä¸¤ä½å°æ•°ï¼Œä¸å››èˆäº”å…¥ï¼›å…è®¸å·®å€¼å·®â‰¤0.01ï¼‰**")
        st.caption(f"ä¸»èƒœ5èµ”ç‡(æˆªæ–­): {odds_ap['H_odds_trunc']} | å­˜åœ¨={odds_ap['H_odds_ap_has']} | ç»„æ•°={odds_ap['H_odds_ap_count']}")
        if odds_ap["H_odds_ap_triplets"]:
            st.caption("ä¸»èƒœä¸‰å…ƒç»„ï¼š " + odds_ap["H_odds_ap_triplets"])

        st.caption(f"å¹³å±€5èµ”ç‡(æˆªæ–­): {odds_ap['D_odds_trunc']} | å­˜åœ¨={odds_ap['D_odds_ap_has']} | ç»„æ•°={odds_ap['D_odds_ap_count']}")
        if odds_ap["D_odds_ap_triplets"]:
            st.caption("å¹³å±€ä¸‰å…ƒç»„ï¼š " + odds_ap["D_odds_ap_triplets"])

        st.caption(f"å®¢èƒœ5èµ”ç‡(æˆªæ–­): {odds_ap['A_odds_trunc']} | å­˜åœ¨={odds_ap['A_odds_ap_has']} | ç»„æ•°={odds_ap['A_odds_ap_count']}")
        if odds_ap["A_odds_ap_triplets"]:
            st.caption("å®¢èƒœä¸‰å…ƒç»„ï¼š " + odds_ap["A_odds_ap_triplets"])

        # å†å²ç›¸ä¼¼ Top5
        curr_pro_res = df_pro.loc[i, "æœ€ç»ˆé¢„æµ‹ç»“æœ"] if "æœ€ç»ˆé¢„æµ‹ç»“æœ" in df_pro.columns else ""
        curr_ens_res = df_ens.loc[i, "PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ"] if "PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ" in df_ens.columns else ""
        curr_pair = f"{curr_pro_res}-{curr_ens_res}" if curr_pro_res and curr_ens_res else ""

        q_basic = {
            "PRO_gap": df_pro.loc[i, "average_gap"],
            "PROèåˆæ¨¡å‹_gap": df_ens.loc[i, "PROèåˆæ¨¡å‹_gap"],
            "èåˆä¿¡å¿ƒ": df_meta.loc[i, "èåˆä¿¡å¿ƒ"] if "èåˆä¿¡å¿ƒ" in df_meta.columns else 0,
            "æ¨èæ€»åˆ†": df_meta.loc[i, "æ¨èæ€»åˆ†"] if "æ¨èæ€»åˆ†" in df_meta.columns else 0,
            "pair": curr_pair
        }

        try:
            sims_basic_full = matcher.query(q_basic, k=5)
        except Exception as e:
            st.warning(f"å†å²åŒ¹é…è°ƒç”¨å‡ºé”™ï¼š{e}")
            sims_basic_full = pd.DataFrame()

        sims_basic_full = sims_basic_full.reset_index(drop=True)
        if sims_basic_full.empty:
            st.info("æœªæ‰¾åˆ°å†å²ç›¸ä¼¼æ¯”èµ›ã€‚")
            continue

        # è®¡ç®— Top5 çš„ PRO / PROèåˆ å·®å€¼æ¨¡å¼
        pro_patterns = compute_gap_patterns(sims_basic_full, "PRO_gap")
        ens_patterns = compute_gap_patterns(sims_basic_full, "PROèåˆæ¨¡å‹_gap")

        pro0 = pro_patterns.get("0-1-2", "")
        pro1 = pro_patterns.get("1-2-3", "")
        pro2 = pro_patterns.get("2-3-4", "")

        ens0 = ens_patterns.get("0-1-2", "")
        ens1 = ens_patterns.get("1-2-3", "")
        ens2 = ens_patterns.get("2-3-4", "")

        # ===== æ–°å¢ï¼š199åº“Top5åºåˆ—åŒ¹é…ï¼ˆ>=4/5ï¼‰=====
        # è·å–å½“å‰Top5åºåˆ—ï¼ˆä¼˜å…ˆæ¯”èµ›åºå·ï¼Œå¦åˆ™æ¯”èµ›ç¼–å·ï¼‰
        id_col = "æ¯”èµ›åºå·" if "æ¯”èµ›åºå·" in sims_basic_full.columns else ("æ¯”èµ›ç¼–å·" if "æ¯”èµ›ç¼–å·" in sims_basic_full.columns else None)
        new_seq: List[int] = []
        if id_col is not None:
            new_seq = pd.to_numeric(sims_basic_full[id_col], errors="coerce").dropna().astype(int).tolist()[:5]

        if len(new_seq) == 5 and ref_map_199:
            matches_199 = match_top5_sequence(new_seq, ref_map_199)
            if matches_199:
                st.markdown("**ğŸ” 199åº“ Top5 åºåˆ—åŒ¹é…å‘½ä¸­ï¼ˆâ‰¥4/5 ä¸”é¡ºåºä¸€è‡´ï¼‰**")
                show_df = pd.DataFrame([{
                    "å‘½ä¸­ç­‰çº§": f"{m['level']}/5",
                    "å‚è€ƒåº“_å½“å‰æ¯”èµ›åºå·": m["ref_match_no"],
                    "å‚è€ƒåº“_Top5åºåˆ—": "-".join(map(str, m["ref_seq"])),
                    "å‘½ä¸­çš„4åºåˆ—(è‹¥4/5)": "" if m["matched_subseq"] is None else "-".join(map(str, m["matched_subseq"]))
                } for m in matches_199])
                st.dataframe(show_df, use_container_width=True)
            else:
                st.caption("199åº“åºåˆ—åŒ¹é…ï¼šæœªå‘½ä¸­ â‰¥4/5ã€‚")
        else:
            st.caption("199åº“åºåˆ—åŒ¹é…ï¼šå½“å‰Top5åºåˆ—ä¸è¶³5ä¸ªæˆ–199åº“æœªåŠ è½½ã€‚")

        # æ˜¾ç¤º Top5 è¡¨ï¼ˆæ–°å¢ gap_sum_100 åˆ—ï¼‰
        sims_show = sims_basic_full.copy()

        if "æ¯”èµ›åºå·" not in sims_show.columns:
            if "æ¯”èµ›ç¼–å·" in sims_show.columns:
                sims_show["æ¯”èµ›åºå·"] = sims_show["æ¯”èµ›ç¼–å·"]
            else:
                sims_show["æ¯”èµ›åºå·"] = ""

        if "æ¯”èµ›ç»“æœ" not in sims_show.columns:
            if "æ¯”èµ›ç»“æœ_y" in sims_show.columns:
                sims_show["æ¯”èµ›ç»“æœ"] = sims_show["æ¯”èµ›ç»“æœ_y"]
            elif "æ¯”èµ›ç»“æœ_x" in sims_show.columns:
                sims_show["æ¯”èµ›ç»“æœ"] = sims_show["æ¯”èµ›ç»“æœ_x"]
            else:
                sims_show["æ¯”èµ›ç»“æœ"] = ""

        for col in ["PRO_æœ€ç»ˆé¢„æµ‹ç»“æœ", "PRO_gap", "PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ", "PROèåˆæ¨¡å‹_gap"]:
            if col not in sims_show.columns:
                sims_show[col] = ""

        sims_show["gap_sum_100"] = sims_show.apply(
            lambda r: compute_gap_sum_100(r.get("PRO_gap", 0.0), r.get("PROèåˆæ¨¡å‹_gap", 0.0)),
            axis=1
        )

        sims_show = sims_show[[
            "æ¯”èµ›åºå·", "æ¯”èµ›ç»“æœ",
            "PRO_æœ€ç»ˆé¢„æµ‹ç»“æœ", "PRO_gap",
            "PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ", "PROèåˆæ¨¡å‹_gap",
            "gap_sum_100"
        ]]

        sims_show["PRO_gap"] = pd.to_numeric(sims_show["PRO_gap"], errors="coerce").round(4)
        sims_show["PROèåˆæ¨¡å‹_gap"] = pd.to_numeric(sims_show["PROèåˆæ¨¡å‹_gap"], errors="coerce").round(4)

        render_compact_table(sims_show)

        # æ¨¡å¼è¡¨
        st.markdown("**å·®å€¼æ¨¡å¼ï¼ˆåŸºäºå†å² Top5ï¼‰**")
        st.markdown(f"""
<table class="pattern-table">
  <tr><th></th><th>0-1-2</th><th>1-2-3</th><th>2-3-4</th></tr>
  <tr><th>PRO_gap</th><td>{pro0}</td><td>{pro1}</td><td>{pro2}</td></tr>
  <tr><th>PROèåˆæ¨¡å‹_gap</th><td>{ens0}</td><td>{ens1}</td><td>{ens2}</td></tr>
</table>
""", unsafe_allow_html=True)

        # æ¨¡å¼è®¡æ•°
        counts = compute_pattern_counts_for_match(pro0, pro1, pro2, ens0, ens1, ens2)
        st.caption(
            f"è®¡æ•°ç»“æœï¼šPRO equal={counts['equal_pro']}, diffÂ±1={counts['diff1_pro']}ï¼›"
            f"ENS equal={counts['equal_ens']}, diffÂ±1={counts['diff1_ens']}ï¼›"
            f"Cross equal={counts['equal_cross']}, diffÂ±1={counts['diff1_cross']}ï¼›"
            f"Total={counts['total_count']}, parity={counts['parity']}"
        )

        # gap ç­‰å·®ä¸‰å…ƒç»„ï¼ˆPRO_gapï¼‰
        pro_trunc_list, pro_has_ap, pro_best_triplet, pro_all_triplets = compute_gap_ap(sims_basic_full, "PRO_gap")
        if pro_has_ap and pro_best_triplet:
            st.caption("PRO_gap ç­‰å·®ä¸‰å…ƒç»„(æœ€å°å…¬å·®)ï¼š " + "ã€".join(f"{v:.2f}" for v in pro_best_triplet))
            if len(pro_all_triplets) > 1:
                st.caption("PRO_gap å…¨éƒ¨ç­‰å·®ä¸‰å…ƒç»„ï¼š " + "ï¼› ".join("ã€".join(f"{v:.2f}" for v in t) for t in pro_all_triplets))
        else:
            st.caption("PRO_gapï¼šæ— ç­‰å·®ä¸‰å…ƒç»„")

        # gap ç­‰å·®ä¸‰å…ƒç»„ï¼ˆPROèåˆæ¨¡å‹_gapï¼‰
        ens_trunc_list, ens_has_ap, ens_best_triplet, ens_all_triplets = compute_gap_ap(sims_basic_full, "PROèåˆæ¨¡å‹_gap")
        if ens_has_ap and ens_best_triplet:
            st.caption("PROèåˆæ¨¡å‹_gap ç­‰å·®ä¸‰å…ƒç»„(æœ€å°å…¬å·®)ï¼š " + "ã€".join(f"{v:.2f}" for v in ens_best_triplet))
            if len(ens_all_triplets) > 1:
                st.caption("PROèåˆæ¨¡å‹_gap å…¨éƒ¨ç­‰å·®ä¸‰å…ƒç»„ï¼š " + "ï¼› ".join("ã€".join(f"{v:.2f}" for v in t) for t in ens_all_triplets))
        else:
            st.caption("PROèåˆæ¨¡å‹_gapï¼šæ— ç­‰å·®ä¸‰å…ƒç»„")

        # ===== å¯¼å‡ºè¡Œï¼ˆä¿æŒåŸé€»è¾‘ï¼‰=====
        hist_home_col = "ä¸»é˜Ÿ" if "ä¸»é˜Ÿ" in sims_basic_full.columns else None
        hist_away_col = "å®¢é˜Ÿ" if "å®¢é˜Ÿ" in sims_basic_full.columns else None

        pro_best_triplet_str = "|".join(f"{v:.2f}" for v in pro_best_triplet) if pro_best_triplet else ""
        pro_all_triplets_str = ";".join("|".join(f"{v:.2f}" for v in t) for t in pro_all_triplets) if pro_all_triplets else ""

        ens_best_triplet_str = "|".join(f"{v:.2f}" for v in ens_best_triplet) if ens_best_triplet else ""
        ens_all_triplets_str = ";".join("|".join(f"{v:.2f}" for v in t) for t in ens_all_triplets) if ens_all_triplets else ""

        for _, r in sims_basic_full.iterrows():
            hist_pro_gap = r.get("PRO_gap", 0.0)
            hist_ens_gap = r.get("PROèåˆæ¨¡å‹_gap", 0.0)
            export_rows.append({
                "å½“å‰æ¯”èµ›åºå·": i + 1,
                "å½“å‰ä¸»é˜Ÿ": home,
                "å½“å‰å®¢é˜Ÿ": away,

                "å†å²æ¯”èµ›åºå·": r.get("æ¯”èµ›åºå·", r.get("æ¯”èµ›ç¼–å·", "")),
                "å†å²æ¯”èµ›ç»“æœ": get_result_value(r),
                "å†å²PRO_æœ€ç»ˆé¢„æµ‹ç»“æœ": r.get("PRO_æœ€ç»ˆé¢„æµ‹ç»“æœ", ""),
                "å†å²PRO_gap": hist_pro_gap,
                "å†å²PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ": r.get("PROèåˆæ¨¡å‹é¢„æµ‹ç»“æœ", ""),
                "å†å²PROèåˆæ¨¡å‹_gap": hist_ens_gap,
                "å†å²gap_sum_100": compute_gap_sum_100(hist_pro_gap, hist_ens_gap),

                "å†å²ä¸»é˜Ÿ": r.get(hist_home_col, "") if hist_home_col else "",
                "å†å²å®¢é˜Ÿ": r.get(hist_away_col, "") if hist_away_col else "",

                "equal_pro": counts["equal_pro"],
                "diff1_pro": counts["diff1_pro"],
                "equal_ens": counts["equal_ens"],
                "diff1_ens": counts["diff1_ens"],
                "equal_cross": counts["equal_cross"],
                "diff1_cross": counts["diff1_cross"],
                "total_count": counts["total_count"],
                "parity": counts["parity"],

                "PRO_gap_top5_trunc": ",".join(f"{v:.2f}" for v in pro_trunc_list),
                "PRO_gap_has_ap": pro_has_ap,
                "PRO_gap_ap_triplet": pro_best_triplet_str,
                "PRO_gap_ap_triplets_all": pro_all_triplets_str,

                "ENS_gap_top5_trunc": ",".join(f"{v:.2f}" for v in ens_trunc_list),
                "ENS_gap_has_ap": ens_has_ap,
                "ENS_gap_ap_triplet": ens_best_triplet_str,
                "ENS_gap_ap_triplets_all": ens_all_triplets_str,

                # èµ”ç‡è¿‘ä¼¼ç­‰å·®ï¼ˆä¸»/å¹³/å®¢ï¼‰
                "H_odds_trunc": odds_ap["H_odds_trunc"],
                "H_odds_ap_has": odds_ap["H_odds_ap_has"],
                "H_odds_ap_count": odds_ap["H_odds_ap_count"],
                "H_odds_ap_triplets": odds_ap["H_odds_ap_triplets"],

                "D_odds_trunc": odds_ap["D_odds_trunc"],
                "D_odds_ap_has": odds_ap["D_odds_ap_has"],
                "D_odds_ap_count": odds_ap["D_odds_ap_count"],
                "D_odds_ap_triplets": odds_ap["D_odds_ap_triplets"],

                "A_odds_trunc": odds_ap["A_odds_trunc"],
                "A_odds_ap_has": odds_ap["A_odds_ap_has"],
                "A_odds_ap_count": odds_ap["A_odds_ap_count"],
                "A_odds_ap_triplets": odds_ap["A_odds_ap_triplets"],
            })

    # ===== å¯¼å‡ºï¼ˆæ’å…¥ç©ºè¡Œåˆ†ç»„ï¼‰=====
    if export_rows:
        df_export = pd.DataFrame(export_rows)

        for col in ["å†å²PRO_gap", "å†å²PROèåˆæ¨¡å‹_gap"]:
            if col in df_export.columns:
                df_export[col] = pd.to_numeric(df_export[col], errors="coerce").round(4)

        rows_with_blank = []
        last_match = None
        for _, row in df_export.iterrows():
            match_no = row["å½“å‰æ¯”èµ›åºå·"]
            if last_match is not None and match_no != last_match:
                rows_with_blank.append({col: "" for col in df_export.columns})
            rows_with_blank.append(row.to_dict())
            last_match = match_no

        df_export_with_blank = pd.DataFrame(rows_with_blank, columns=df_export.columns)

        st.subheader("ğŸ“¤ å¯¼å‡ºæ‰€æœ‰æ¯”èµ›çš„å†å²ç›¸ä¼¼ Top5ï¼ˆå« gap_sum_100ï¼‰")
        render_compact_table(df_export_with_blank.head(20))

        st.download_button(
            "â¬‡ï¸ å¯¼å‡ºå†å²ç›¸ä¼¼ Top5ï¼ˆCSVï¼‰",
            df_export_with_blank.to_csv(index=False).encode("utf-8-sig"),
            "all_matches_top5_history_with_gap_sum_100.csv",
            "text/csv",
        )
